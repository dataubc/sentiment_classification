{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hotel Review Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mosal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import StringIO\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler, PowerTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout,MaxPooling1D,Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM,Bidirectional\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# from spacy.cli import download\n",
    "# print(download('en')\n",
    "#\n",
    "#nlp = spacy.load('en')\n",
    "\n",
    "#import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for reading data from the url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_train = pd.read_csv('Data/sentiment_dataset_train.csv')\n",
    "hotel_dev = pd.read_csv('Data/sentiment_dataset_dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Arrived about 10pm and check in was painless. ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I checked in at 4pm even tough room was not re...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I chose this hotel, as it was in a good locati...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Great location, super close to shops &amp; a 10min...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I was in the Sir Adam Hotel to visit a friend....</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             review rating\n",
       "0   0  Arrived about 10pm and check in was painless. ...      4\n",
       "1   1  I checked in at 4pm even tough room was not re...      2\n",
       "2   2  I chose this hotel, as it was in a good locati...      2\n",
       "3   3  Great location, super close to shops & a 10min...      4\n",
       "4   4  I was in the Sir Adam Hotel to visit a friend....      3"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotel_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Arrived about 10pm and check in was painless.   The only downside to this hotel is if you are looking for a city centre location. If you don't mind some walking and want to be out of the noise of the city then this place is ideal.   Hotel has a bar and restaurant, decent size gym and roof terrace with sun loungers.   The rooms are a good size, especially when traveling with a large teenager. Good sized lounge with double sofa bed, kitchen area and dining table. Main bedroom is a good size with double wardrobes and safe. Shower room is well sized with plenty of towels, good supply of toiletries, and hairdryer.   Fridge comes stocked with bottles of water to get you started and you can get more at the hotel bar  A 5 minute walk takes you to Marina metro station and your access to…\""
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotel_train.loc[0,'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35005, 3)"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotel_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2                                                7031\n",
       "1                                                7028\n",
       "4                                                6997\n",
       "5                                                6977\n",
       "3                                                6971\n",
       "Tables not made up prior to guest seating. 2.       1\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotel_train['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_train = hotel_train[hotel_train['rating'] !='Tables not made up prior to guest seating. 2.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, \n",
    "               min_token_len = 2, \n",
    "               irrelevant_pos = ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE']): \n",
    "    \"\"\"\n",
    "    Given text, min_token_len, and irrelevant_pos carry out preprocessing of the text \n",
    "    and return a preprocessed string. \n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    text : (str) \n",
    "        the text to be preprocessed\n",
    "    min_token_len : (int) \n",
    "        min_token_length required\n",
    "    irrelevant_pos : (list) \n",
    "        a list of irrelevant pos tags\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    (str) the preprocessed text\n",
    "    \"\"\"\n",
    "    #cleaning the data\n",
    "    text=text.replace(r'\\n','')\n",
    "    stop_words = list(set(stopwords.words('english')))+list(string.punctuation)  # all stop_words and punctuation\n",
    "    stop_words.extend(['``','’', '`','br','\"',\"”\", \"''\", \"'s\",'the','hotel'])\n",
    "    tokens = word_tokenize(text)\n",
    "    processed_text = [] \n",
    "    for token in tokens:\n",
    "        if token.lower() not in stop_words:\n",
    "          processed_text.append(str(token.lower()))\n",
    "    final_txt = \" \".join(processed_text)\n",
    "    return final_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_train['cleaned'] = hotel_train['review'].apply(preprocess)\n",
    "hotel_dev['cleaned'] = hotel_dev['review'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Arrived about 10pm and check in was painless. ...</td>\n",
       "      <td>4</td>\n",
       "      <td>arrived 10pm check painless downside looking c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I checked in at 4pm even tough room was not re...</td>\n",
       "      <td>2</td>\n",
       "      <td>checked 4pm even tough room ready .. staff bus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I chose this hotel, as it was in a good locati...</td>\n",
       "      <td>2</td>\n",
       "      <td>chose good location room bath spa available ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Great location, super close to shops &amp; a 10min...</td>\n",
       "      <td>4</td>\n",
       "      <td>great location super close shops 10min walk ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I was in the Sir Adam Hotel to visit a friend....</td>\n",
       "      <td>3</td>\n",
       "      <td>sir adam visit friend enjoyed time bar differe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35000</th>\n",
       "      <td>35000</td>\n",
       "      <td>Paris is always welcome city, but this time th...</td>\n",
       "      <td>5</td>\n",
       "      <td>paris always welcome city time stay concortel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35001</th>\n",
       "      <td>35001</td>\n",
       "      <td>Beautiful place very clean and welcoming irini...</td>\n",
       "      <td>3</td>\n",
       "      <td>beautiful place clean welcoming irini wonderfu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35002</th>\n",
       "      <td>35002</td>\n",
       "      <td>The hotel is ok considering the price we paid....</td>\n",
       "      <td>3</td>\n",
       "      <td>ok considering price paid breakfast could leas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35003</th>\n",
       "      <td>35003</td>\n",
       "      <td>First your stuck if you miss last tram at midn...</td>\n",
       "      <td>3</td>\n",
       "      <td>first stuck miss last tram midnight cheapest w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35004</th>\n",
       "      <td>35004</td>\n",
       "      <td>The staff was very nice. The room was fine - n...</td>\n",
       "      <td>3</td>\n",
       "      <td>staff nice room fine spectacular small locatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35004 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                             review rating  \\\n",
       "0          0  Arrived about 10pm and check in was painless. ...      4   \n",
       "1          1  I checked in at 4pm even tough room was not re...      2   \n",
       "2          2  I chose this hotel, as it was in a good locati...      2   \n",
       "3          3  Great location, super close to shops & a 10min...      4   \n",
       "4          4  I was in the Sir Adam Hotel to visit a friend....      3   \n",
       "...      ...                                                ...    ...   \n",
       "35000  35000  Paris is always welcome city, but this time th...      5   \n",
       "35001  35001  Beautiful place very clean and welcoming irini...      3   \n",
       "35002  35002  The hotel is ok considering the price we paid....      3   \n",
       "35003  35003  First your stuck if you miss last tram at midn...      3   \n",
       "35004  35004  The staff was very nice. The room was fine - n...      3   \n",
       "\n",
       "                                                 cleaned  \n",
       "0      arrived 10pm check painless downside looking c...  \n",
       "1      checked 4pm even tough room ready .. staff bus...  \n",
       "2      chose good location room bath spa available ar...  \n",
       "3      great location super close shops 10min walk ma...  \n",
       "4      sir adam visit friend enjoyed time bar differe...  \n",
       "...                                                  ...  \n",
       "35000  paris always welcome city time stay concortel ...  \n",
       "35001  beautiful place clean welcoming irini wonderfu...  \n",
       "35002  ok considering price paid breakfast could leas...  \n",
       "35003  first stuck miss last tram midnight cheapest w...  \n",
       "35004  staff nice room fine spectacular small locatio...  \n",
       "\n",
       "[35004 rows x 4 columns]"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotel_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hotel_train['cleaned']\n",
    "y_train = hotel_train['rating']\n",
    "\n",
    "X_dev = hotel_dev['cleaned']\n",
    "y_dev = hotel_dev['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        arrived 10pm check painless downside looking c...\n",
       "1        checked 4pm even tough room ready .. staff bus...\n",
       "2        chose good location room bath spa available ar...\n",
       "3        great location super close shops 10min walk ma...\n",
       "4        sir adam visit friend enjoyed time bar differe...\n",
       "                               ...                        \n",
       "35000    paris always welcome city time stay concortel ...\n",
       "35001    beautiful place clean welcoming irini wonderfu...\n",
       "35002    ok considering price paid breakfast could leas...\n",
       "35003    first stuck miss last tram midnight cheapest w...\n",
       "35004    staff nice room fine spectacular small locatio...\n",
       "Name: cleaned, Length: 35004, dtype: object"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_dev_vec = vectorizer.transform(X_dev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Naive bays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time for fitting =  0.023942947387695312\n",
      "Train accuracy =  0.8246771797508856\n",
      "dev accuracy =  0.7486331510868116\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "nb = MultinomialNB().fit(X_train_vec, y_train)\n",
    "elapsed_time = time.time() - t\n",
    "print('elapsed time for fitting = ',elapsed_time)\n",
    "print('Train accuracy = ',nb.score(X_train_vec, y_train))\n",
    "print('dev accuracy = ',nb.score(X_dev_vec, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time for fitting =  29.12623119354248\n",
      "Train accuracy =  0.9770311964346932\n",
      "Train accuracy =  0.738498466462195\n"
     ]
    }
   ],
   "source": [
    "# fitting the model\n",
    "t = time.time()\n",
    "lgr = LogisticRegression(max_iter=5000, penalty='l2', random_state=None,  solver='lbfgs').fit(X_train_vec, y_train)\n",
    "elapsed_time = time.time() - t\n",
    "print('elapsed time for fitting = ',elapsed_time)\n",
    "# printing the result\n",
    "print('Train accuracy = ',lgr.score(X_train_vec, y_train))\n",
    "print('Train accuracy = ',lgr.score(X_dev_vec, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time for fitting =  1.5714919567108154\n",
      "Train accuracy =  0.9422637412867101\n",
      "Train accuracy =  0.7375650086678224\n"
     ]
    }
   ],
   "source": [
    "# fitting the model\n",
    "t = time.time()\n",
    "sgd =SGDClassifier().fit(X_train_vec, y_train)\n",
    "elapsed_time = time.time() - t\n",
    "print('elapsed time for fitting = ',elapsed_time)\n",
    "# printing the result\n",
    "print('Train accuracy = ',sgd.score(X_train_vec, y_train))\n",
    "print('Train accuracy = ',sgd.score(X_dev_vec, y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLTextClassifier():\n",
    "    def __init__(self, \n",
    "                 embedding_dimension = 200,\n",
    "                 max_features = 20000, \n",
    "                 maxlen = 80):\n",
    "        \"\"\"\n",
    "        Instantiate the DLTextClassifer.              \n",
    "        Parameters:\n",
    "        ------------------        \n",
    "        embedding_dimension : (int)\n",
    "            size of your word embedding vector\n",
    "        max_features: (int)\n",
    "            max number of words to keep in the vocabulary\n",
    "        maxlen : (int)\n",
    "            sequence length\n",
    "        \"\"\"\n",
    "        # We'll be using an embedding layer and pass a vector of\n",
    "        # size embedding dimension instead of one-hot-encoding. \n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.max_features = max_features    \n",
    "        # Sequence length\n",
    "        self.maxlen = maxlen\n",
    "        # Create the tokenizer. We'll be using Keras Tokenizer here.         \n",
    "        self.tokenizer = Tokenizer(num_words=self.max_features, \n",
    "                             filters='! #$% ()*+,-./:; = ?@[\\\\]^_`{|}~\\t\\n>\"<') \n",
    "        # Store word_index\n",
    "        self.word_index = self.tokenizer.word_index\n",
    "                \n",
    "    def prepare_data(self, corpus, mode = 'train'):\n",
    "        \"\"\"\n",
    "        Create a mapping from the unique words in the training corpus to integers, \n",
    "        the output then passed to pad_sequences function to give all the sequence\n",
    "        the same length. \n",
    "\n",
    "        Parameters:\n",
    "        ------------------ \n",
    "        corpus :  list of texts\n",
    "        mode : train or test\n",
    "\n",
    "        Returns:\n",
    "        ------------------ \n",
    "        padded sequences of the corpus\n",
    "        print the length of the encoded corpus\n",
    "        \"\"\"\n",
    "        if mode == 'train': \n",
    "            # fit the tokenizer on the documents\n",
    "            self.tokenizer.fit_on_texts(corpus)\n",
    "        \n",
    "        # integer encoded documents\n",
    "        encoded_corpus = self.tokenizer.texts_to_sequences(corpus)        \n",
    "        print('len of encoded docs: ', len(encoded_corpus))\n",
    "        return self.pad_sequences(encoded_corpus)\n",
    "\n",
    "    def pad_sequences(self, data):\n",
    "        \"\"\"\n",
    "        This function will call pad_sequences function to add 0 to the sequence\n",
    "        in the data that has shorter length than the maximum length and print the\n",
    "        shape of the padded data.\n",
    "\n",
    "        Parameters:\n",
    "        ------------------ \n",
    "        data: List of lists and each element is a sequence\n",
    "        Returns :\n",
    "        ------------------ \n",
    "        padded data : Array of the padded data    \n",
    "        \"\"\"\n",
    "       \n",
    "        print('Pad sequences (samples x time)')\n",
    "        padded_data = sequence.pad_sequences(data, maxlen=self.maxlen)\n",
    "        print('Padded data shape:', padded_data.shape)\n",
    "        return padded_data    \n",
    "          \n",
    "    def build_network(self, layer_size = 256, dropout_amount=0.4):\n",
    "        \"\"\"\n",
    "        Given layer_size and dropout_amount, build an LSTM network\n",
    "        using Keras and tensorflow and print summary of the model. \n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        layer_size : int\n",
    "          The number of units to be passed in the LSTM layer\n",
    "        dropout_amount : float\n",
    "          the dropout amount to be passed in the Dropout layer. \n",
    "\n",
    "        Return\n",
    "        -----------\n",
    "        None\n",
    "          print the summary of the model \n",
    "        \"\"\"      \n",
    "    \n",
    "        model = Sequential()\n",
    "        model.add(Embedding(self.max_features, self.embedding_dimension,\n",
    "                            input_length=self.maxlen))\n",
    "        model.add(LSTM(layer_size, dropout=dropout_amount,\n",
    "                       recurrent_dropout=dropout_amount,return_sequences=True))\n",
    "        model.add(LSTM(layer_size,return_sequences=True))\n",
    "        model.add(LSTM(layer_size))\n",
    "        model.add(Dense(1, activation='sigmoid')) \n",
    "        model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "        self.model=model\n",
    "        print(model.summary())\n",
    "               \n",
    "    def fit(self, \n",
    "              X_train, y_train,\n",
    "              batch_size =32, \n",
    "              epochs = 10, \n",
    "              save_path='../data/'):\n",
    "        \"\"\"        \n",
    "        Given the parameters train a deep learning model and save and return it.  \n",
    "        \n",
    "        Parameters\n",
    "        -------------\n",
    "        X_train : (list) \n",
    "          the X values of the train split \n",
    "        y_train : (list) \n",
    "          the y values of the train split \n",
    "        batch_size : (int) \n",
    "          the batch_size for the training\n",
    "        epochs : (int) \n",
    "          the number of epochs for training \n",
    "        save_path : (str) the path to save the model\n",
    "        \n",
    "        Return\n",
    "        -------------\n",
    "          the trained model\n",
    "        \"\"\"      \n",
    "        # Directory where the checkpoints will be saved\n",
    "        checkpoint_dir = save_path\n",
    "        # Name of the checkpoint files\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "        checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_prefix,\n",
    "            save_weights_only=True)\n",
    "        X_train_padded = self.prepare_data(X_train, mode='train')\n",
    "        self.model.fit(X_train_padded, y_train,epochs=epochs, \n",
    "         batch_size=batch_size, \n",
    "         callbacks=[checkpoint_callback])\n",
    "        self.model.save(save_path)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Given a model and X_test, y_test, evaluates the model and prints \n",
    "        the accuracy\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X_test -- Array of text data\n",
    "        y_test -- labels for every text\n",
    "        \n",
    "        Return:    \n",
    "        -----------    \n",
    "        None                \n",
    "        \"\"\"        \n",
    "        X_test_padded = self.prepare_data(X_test, mode='test')        \n",
    "        score, acc = self.model.evaluate(X_test_padded, y_test)\n",
    "        print('Accuracy: ', acc)\n",
    "\n",
    "\n",
    "    def predict(self, texts):\n",
    "        \"\"\"\n",
    "        Return predicted labels for a list of texts.        \n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        \n",
    "        texts : (list) \n",
    "          a list of text comments\n",
    "\n",
    "        Return:\n",
    "        --------        \n",
    "          a list of prediction scores        \n",
    "        \"\"\"        \n",
    "        padded_sequences = self.prepare_data(texts, mode = 'test')\n",
    "        return self.model.predict(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 80, 200)           4000000   \n",
      "_________________________________________________________________\n",
      "lstm_36 (LSTM)               (None, 80, 256)           467968    \n",
      "_________________________________________________________________\n",
      "lstm_37 (LSTM)               (None, 80, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_38 (LSTM)               (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 5,518,849\n",
      "Trainable params: 5,518,849\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "len of encoded docs:  35004\n",
      "Pad sequences (samples x time)\n",
      "Padded data shape: (35004, 80)\n",
      "Epoch 1/100\n",
      "274/274 [==============================] - 31530s 115s/step - loss: -98.5025 - accuracy: 0.2005\n",
      "Epoch 2/100\n",
      "274/274 [==============================] - 282s 1s/step - loss: -231.3074 - accuracy: 0.2008\n",
      "Epoch 3/100\n",
      "168/274 [=================>............] - ETA: 48:17 - loss: -337.8592 - accuracy: 0.2028"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-389-eb48cea091f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melapsed_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-368-069715acc148>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, batch_size, epochs, save_path)\u001b[0m\n\u001b[1;32m    135\u001b[0m             save_weights_only=True)\n\u001b[1;32m    136\u001b[0m         \u001b[0mX_train_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         self.model.fit(X_train_padded, y_train,epochs=epochs, \n\u001b[0m\u001b[1;32m    138\u001b[0m          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m          callbacks=[checkpoint_callback])\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_p =DLTextClassifier()\n",
    "model_p.build_network()\n",
    "t = time.time()\n",
    "model_p.fit(X_train,y_train,batch_size =128,epochs = 100)\n",
    "elapsed_time = time.time() - t\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
