{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hotel Review Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mosal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import StringIO\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler, PowerTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout,MaxPooling1D,Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM,Bidirectional\n",
    "from keras.layers.convolutional import Conv1D\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# from spacy.cli import download\n",
    "# print(download('en')\n",
    "#\n",
    "#nlp = spacy.load('en')\n",
    "\n",
    "#import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for reading data from the url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_train = pd.read_csv('Data/sentiment_dataset_train.csv')\n",
    "hotel_dev = pd.read_csv('Data/sentiment_dataset_dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_train.head()\n",
    "hotel_train = hotel_train[hotel_train['rating'] !='Tables not made up prior to guest seating. 2.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35004, 3)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotel_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_train = hotel_train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hotel_train['review']\n",
    "y_train = hotel_train['rating']\n",
    "\n",
    "X_dev = hotel_dev['review']\n",
    "y_dev = hotel_dev['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Arrived about 10pm and check in was painless. ...\n",
       "1        I checked in at 4pm even tough room was not re...\n",
       "2        I chose this hotel, as it was in a good locati...\n",
       "3        Great location, super close to shops & a 10min...\n",
       "4        I was in the Sir Adam Hotel to visit a friend....\n",
       "                               ...                        \n",
       "35000    Paris is always welcome city, but this time th...\n",
       "35001    Beautiful place very clean and welcoming irini...\n",
       "35002    The hotel is ok considering the price we paid....\n",
       "35003    First your stuck if you miss last tram at midn...\n",
       "35004    The staff was very nice. The room was fine - n...\n",
       "Name: review, Length: 35004, dtype: object"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train).toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        4\n",
       "1        2\n",
       "2        2\n",
       "3        4\n",
       "4        3\n",
       "        ..\n",
       "35000    5\n",
       "35001    3\n",
       "35002    3\n",
       "35003    3\n",
       "35004    3\n",
       "Name: rating, Length: 35005, dtype: object"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = hotel_train['rating']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model\n",
    "lgr = LogisticRegression(max_iter=500, penalty='l2', random_state=None,  solver='lbfgs').fit(X_train_vec, y_train)\n",
    "# printing the result\n",
    "print('Train accuracy = ',lgr.score(X_train_vec, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "class DLTextClassifier():\n",
    "    def __init__(self, \n",
    "                 embedding_dimension = 200,\n",
    "                 max_features = 20000, \n",
    "                 maxlen = 80):\n",
    "        \"\"\"\n",
    "        Instantiate the DLTextClassifer.              \n",
    "        Parameters:\n",
    "        ------------------        \n",
    "        embedding_dimension : (int)\n",
    "            size of your word embedding vector\n",
    "        max_features: (int)\n",
    "            max number of words to keep in the vocabulary\n",
    "        maxlen : (int)\n",
    "            sequence length\n",
    "        \"\"\"\n",
    "        # We'll be using an embedding layer and pass a vector of\n",
    "        # size embedding dimension instead of one-hot-encoding. \n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.max_features = max_features    \n",
    "        # Sequence length\n",
    "        self.maxlen = maxlen\n",
    "        # Create the tokenizer. We'll be using Keras Tokenizer here.         \n",
    "        self.tokenizer = Tokenizer(num_words=self.max_features, \n",
    "                             filters='! #$% ()*+,-./:; = ?@[\\\\]^_`{|}~\\t\\n>\"<') \n",
    "        # Store word_index\n",
    "        self.word_index = self.tokenizer.word_index\n",
    "                \n",
    "    def prepare_data(self, corpus, mode = 'train'):\n",
    "        \"\"\"\n",
    "        Create a mapping from the unique words in the training corpus to integers, \n",
    "        the output then passed to pad_sequences function to give all the sequence\n",
    "        the same length. \n",
    "\n",
    "        Parameters:\n",
    "        ------------------ \n",
    "        corpus :  list of texts\n",
    "        mode : train or test\n",
    "\n",
    "        Returns:\n",
    "        ------------------ \n",
    "        padded sequences of the corpus\n",
    "        print the length of the encoded corpus\n",
    "        \"\"\"\n",
    "        if mode == 'train': \n",
    "            # fit the tokenizer on the documents\n",
    "            self.tokenizer.fit_on_texts(corpus)\n",
    "        \n",
    "        # integer encoded documents\n",
    "        encoded_corpus = self.tokenizer.texts_to_sequences(corpus)        \n",
    "        print('len of encoded docs: ', len(encoded_corpus))\n",
    "        return self.pad_sequences(encoded_corpus)\n",
    "\n",
    "    def pad_sequences(self, data):\n",
    "        \"\"\"\n",
    "        This function will call pad_sequences function to add 0 to the sequence\n",
    "        in the data that has shorter length than the maximum length and print the\n",
    "        shape of the padded data.\n",
    "\n",
    "        Parameters:\n",
    "        ------------------ \n",
    "        data: List of lists and each element is a sequence\n",
    "        Returns :\n",
    "        ------------------ \n",
    "        padded data : Array of the padded data    \n",
    "        \"\"\"\n",
    "       \n",
    "        print('Pad sequences (samples x time)')\n",
    "        padded_data = sequence.pad_sequences(data, maxlen=self.maxlen)\n",
    "        print('Padded data shape:', padded_data.shape)\n",
    "        return padded_data    \n",
    "          \n",
    "    def build_network(self, layer_size = 256, dropout_amount=0.5):\n",
    "        \"\"\"\n",
    "        Given layer_size and dropout_amount, build an LSTM network\n",
    "        using Keras and tensorflow and print summary of the model. \n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        layer_size : int\n",
    "          The number of units to be passed in the LSTM layer\n",
    "        dropout_amount : float\n",
    "          the dropout amount to be passed in the Dropout layer. \n",
    "\n",
    "        Return\n",
    "        -----------\n",
    "        None\n",
    "          print the summary of the model \n",
    "        \"\"\"      \n",
    "    \n",
    "        model = Sequential()\n",
    "        model.add(Embedding(self.max_features, self.embedding_dimension,\n",
    "                            input_length=self.maxlen))\n",
    "        model.add(LSTM(layer_size, dropout=dropout_amount,\n",
    "                       recurrent_dropout=dropout_amount,return_sequences=True))\n",
    "        model.add(LSTM(layer_size,return_sequences=True))\n",
    "        model.add(LSTM(layer_size))\n",
    "        model.add(Dense(1, activation='sigmoid')) \n",
    "        model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "        self.model=model\n",
    "        print(model.summary())\n",
    "               \n",
    "    def fit(self, \n",
    "              X_train, y_train,\n",
    "              batch_size =32, \n",
    "              epochs = 10, \n",
    "              save_path='/content/gdrive/My Drive/'):\n",
    "        \"\"\"        \n",
    "        Given the parameters train a deep learning model and save and return it.  \n",
    "        \n",
    "        Parameters\n",
    "        -------------\n",
    "        X_train : (list) \n",
    "          the X values of the train split \n",
    "        y_train : (list) \n",
    "          the y values of the train split \n",
    "        batch_size : (int) \n",
    "          the batch_size for the training\n",
    "        epochs : (int) \n",
    "          the number of epochs for training \n",
    "        save_path : (str) the path to save the model\n",
    "        \n",
    "        Return\n",
    "        -------------\n",
    "          the trained model\n",
    "        \"\"\"      \n",
    "        # Directory where the checkpoints will be saved\n",
    "        checkpoint_dir = save_path\n",
    "        # Name of the checkpoint files\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "        checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_prefix,\n",
    "            save_weights_only=True)\n",
    "        X_train_padded = self.prepare_data(X_train, mode='train')\n",
    "        self.model.fit(X_train_padded, y_train,epochs=epochs, \n",
    "         batch_size=batch_size, \n",
    "         callbacks=[checkpoint_callback])\n",
    "        self.model.save(save_path)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Given a model and X_test, y_test, evaluates the model and prints \n",
    "        the accuracy\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X_test -- Array of text data\n",
    "        y_test -- labels for every text\n",
    "        \n",
    "        Return:    \n",
    "        -----------    \n",
    "        None                \n",
    "        \"\"\"        \n",
    "        X_test_padded = self.prepare_data(X_test, mode='test')        \n",
    "        score, acc = self.model.evaluate(X_test_padded, y_test)\n",
    "        print('Accuracy: ', acc)\n",
    "\n",
    "\n",
    "    def predict(self, texts):\n",
    "        \"\"\"\n",
    "        Return predicted labels for a list of texts.        \n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        \n",
    "        texts : (list) \n",
    "          a list of text comments\n",
    "\n",
    "        Return:\n",
    "        --------        \n",
    "          a list of prediction scores        \n",
    "        \"\"\"        \n",
    "        padded_sequences = self.prepare_data(texts, mode = 'test')\n",
    "        return self.model.predict(padded_sequences)\n",
    "### END STARTER CODE    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE\n",
    "def preprocess(text, \n",
    "               min_token_len = 2, \n",
    "               irrelevant_pos = ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE']): \n",
    "    \"\"\"\n",
    "    Given text, min_token_len, and irrelevant_pos carry out preprocessing of the text \n",
    "    and return a preprocessed string. \n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    text : (str) \n",
    "        the text to be preprocessed\n",
    "    min_token_len : (int) \n",
    "        min_token_length required\n",
    "    irrelevant_pos : (list) \n",
    "        a list of irrelevant pos tags\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    (str) the preprocessed text\n",
    "    \"\"\"\n",
    "    #cleaning the data\n",
    "    text=text.replace(r'\\n','')\n",
    "    stop_words = list(set(stopwords.words('english')))+list(string.punctuation)  # all stop_words and punctuation\n",
    "    stop_words.extend(['``','’', '`','br','\"',\"”\", \"''\", \"'s\"])\n",
    "    tokens = word_tokenize(text)\n",
    "    processed_text = [] \n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "          processed_text.append(str(token))\n",
    "    final_txt = \" \".join(processed_text)\n",
    "    return final_txt\n",
    "### END STARTER CO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-192-3411e0ce6ae4>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['preproc_text'] = X_train.review.apply(preprocess)\n"
     ]
    }
   ],
   "source": [
    "X_train['preproc_text'] = X_train.review.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train['preproc_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Arrived 10pm check painless The downside hotel...\n",
       "1        I checked 4pm even tough room ready .. staff b...\n",
       "2        I chose hotel good location room bath spa avai...\n",
       "3        Great location super close shops 10min walk ma...\n",
       "4        I Sir Adam Hotel visit friend We enjoyed time ...\n",
       "                               ...                        \n",
       "35000    Paris always welcome city time I stay Hotel Co...\n",
       "35001    Beautiful place clean welcoming irini wonderfu...\n",
       "35002    The hotel ok considering price paid Breakfast ...\n",
       "35003    First stuck miss last tram midnight cheapest w...\n",
       "35004    The staff nice The room fine spectacular small...\n",
       "Name: preproc_text, Length: 35005, dtype: object"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Task 2 : Planned/No planned\n",
    "# model_p =DLTextClassifier()\n",
    "# model_p.build_network()\n",
    "# t = time.time()\n",
    "# model_p.fit(X_train,y_train,batch_size =128,epochs = 1)\n",
    "# elapsed_time = time.time() - t\n",
    "# print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
